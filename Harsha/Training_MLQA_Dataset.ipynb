{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPyInGJMEHcf7wLPdjVFuFL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"33f4fd9b534b47558b7e9a9d90eb6d06":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d2b1b6f81664186addcdef0083e7d0c","IPY_MODEL_fd522a00c5194eca96841fc93ade4ccc","IPY_MODEL_820c24a2565e4e05ba276bf602727885"],"layout":"IPY_MODEL_ae72d83203b8400cb5279a4803561a24"}},"9d2b1b6f81664186addcdef0083e7d0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ad83232fb8d6478bb1b5dd3ee1d673a6","placeholder":"​","style":"IPY_MODEL_9def49c537804659afcee9b594f12a2e","value":"tokenizer_config.json: 100%"}},"fd522a00c5194eca96841fc93ade4ccc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67ad5ef68b014259b206624e5f9d6d68","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bbcf00ac21cd4d99b3b719840093c695","value":48}},"820c24a2565e4e05ba276bf602727885":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f42e5e505e1e4ea4be572b18efaad1b1","placeholder":"​","style":"IPY_MODEL_03b28c43ca5442c3bad90d32e1b37b74","value":" 48.0/48.0 [00:00&lt;00:00, 4.92kB/s]"}},"ae72d83203b8400cb5279a4803561a24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad83232fb8d6478bb1b5dd3ee1d673a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9def49c537804659afcee9b594f12a2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67ad5ef68b014259b206624e5f9d6d68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbcf00ac21cd4d99b3b719840093c695":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f42e5e505e1e4ea4be572b18efaad1b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03b28c43ca5442c3bad90d32e1b37b74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ac342301f5f405b972ed5d151bc8f38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7907df54d6b641ae885d38f6fb7d950a","IPY_MODEL_b2c56bda18424a6db3319a6dd695a822","IPY_MODEL_99d74e429fe045edad8cc1b5c804f6d3"],"layout":"IPY_MODEL_ecda7f6a27df4c838159c52eb301d6bd"}},"7907df54d6b641ae885d38f6fb7d950a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c3228c8118c439ea9a615200daa8bb4","placeholder":"​","style":"IPY_MODEL_039f279c9cf04873a1a1be1d52ac657d","value":"config.json: 100%"}},"b2c56bda18424a6db3319a6dd695a822":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6e8bbe4bad94489ab5004ea51bbe199","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40cc997b923d4790bdf6dd2cf1fa94af","value":570}},"99d74e429fe045edad8cc1b5c804f6d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92ece4f366e1457ab43e8f7f001a077f","placeholder":"​","style":"IPY_MODEL_60d41385225a47b3b722b31bdaf4e891","value":" 570/570 [00:00&lt;00:00, 77.1kB/s]"}},"ecda7f6a27df4c838159c52eb301d6bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c3228c8118c439ea9a615200daa8bb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"039f279c9cf04873a1a1be1d52ac657d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6e8bbe4bad94489ab5004ea51bbe199":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40cc997b923d4790bdf6dd2cf1fa94af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"92ece4f366e1457ab43e8f7f001a077f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60d41385225a47b3b722b31bdaf4e891":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f47c2300fae5493f9544c7814e08f2ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c055ffbcbb7d4f92b416fa6e3a36b12d","IPY_MODEL_e07dd857f1584c87a7ec955efffbe195","IPY_MODEL_2773726b8a6446b19c09c5b50e376680"],"layout":"IPY_MODEL_0b8209c89e7c4d04977fb650fb74a035"}},"c055ffbcbb7d4f92b416fa6e3a36b12d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00d451e4468c4a2bbbd2066d0b719efb","placeholder":"​","style":"IPY_MODEL_05730c1bb5d74a8788d5618b3a497363","value":"vocab.txt: 100%"}},"e07dd857f1584c87a7ec955efffbe195":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aa39e39639e471aab16b91fb5062349","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6248b00de0414c70b9cbef1a1fb45b10","value":231508}},"2773726b8a6446b19c09c5b50e376680":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a7dca0eaac242b3aa27bc99aeebec56","placeholder":"​","style":"IPY_MODEL_f93497793afd4169902898a8fbd9c832","value":" 232k/232k [00:00&lt;00:00, 1.86MB/s]"}},"0b8209c89e7c4d04977fb650fb74a035":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00d451e4468c4a2bbbd2066d0b719efb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05730c1bb5d74a8788d5618b3a497363":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8aa39e39639e471aab16b91fb5062349":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6248b00de0414c70b9cbef1a1fb45b10":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a7dca0eaac242b3aa27bc99aeebec56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f93497793afd4169902898a8fbd9c832":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94230ce9c7414a75a46058c698595bcf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_233b6c5c85ec4c40a062e282f0d2c393","IPY_MODEL_ecd4d3cf138b46dd92ff786db5db129f","IPY_MODEL_4f388d46d1814d9790c18636f41b6095"],"layout":"IPY_MODEL_8a906fd8196c4788be96d5a2ee918a6b"}},"233b6c5c85ec4c40a062e282f0d2c393":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e109fd09ead4f279dc7a67b9ed3e925","placeholder":"​","style":"IPY_MODEL_6ada65a8b4ca46d993b80aaff3bc22cc","value":"tokenizer.json: 100%"}},"ecd4d3cf138b46dd92ff786db5db129f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ddba0eb8e5a40299d6650b54c4fc9b4","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87eff7bfe7cb46e38f209ef19555dc8d","value":466062}},"4f388d46d1814d9790c18636f41b6095":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cbefa3392f143f398706bcbd906a52d","placeholder":"​","style":"IPY_MODEL_9a056b6bb92546489a5b784ba70baea2","value":" 466k/466k [00:00&lt;00:00, 6.88MB/s]"}},"8a906fd8196c4788be96d5a2ee918a6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e109fd09ead4f279dc7a67b9ed3e925":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ada65a8b4ca46d993b80aaff3bc22cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ddba0eb8e5a40299d6650b54c4fc9b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87eff7bfe7cb46e38f209ef19555dc8d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cbefa3392f143f398706bcbd906a52d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a056b6bb92546489a5b784ba70baea2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c740255ad56e4e18b3041c580f46e8a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a121671429648bca691040ec24352b2","IPY_MODEL_c4db1d80c6f64acabf3751b7de83ec68","IPY_MODEL_5da96aa9350b4a1f8c58fb7bda919c11"],"layout":"IPY_MODEL_b7f88da3ce92450ba511ad42fe1041a1"}},"2a121671429648bca691040ec24352b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b91cd6346c304dcd9edc03e7a44ad04b","placeholder":"​","style":"IPY_MODEL_5e10db69c14448a69648e3ff839f087b","value":"Map: 100%"}},"c4db1d80c6f64acabf3751b7de83ec68":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e0312b8e5b84cbebcfc2593bee0ed1c","max":11590,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab09bc7b1f5341d1bc8338db11b63fc5","value":11590}},"5da96aa9350b4a1f8c58fb7bda919c11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64188d78030542d4a71081323ad30d5c","placeholder":"​","style":"IPY_MODEL_eb7ea44631b043b2bb34951da8d85faa","value":" 11590/11590 [00:12&lt;00:00, 1077.95 examples/s]"}},"b7f88da3ce92450ba511ad42fe1041a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b91cd6346c304dcd9edc03e7a44ad04b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e10db69c14448a69648e3ff839f087b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e0312b8e5b84cbebcfc2593bee0ed1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab09bc7b1f5341d1bc8338db11b63fc5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"64188d78030542d4a71081323ad30d5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb7ea44631b043b2bb34951da8d85faa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iC8d-WyS7uoR","executionInfo":{"status":"ok","timestamp":1765200688260,"user_tz":-330,"elapsed":27749,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"d9375825-62f8-4582-cda3-479e8b1d72fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["datapath = '/content/drive/MyDrive/datasets/MLQA_V1.zip'\n","import zipfile\n","extract_path = '/content/dataset'\n","with zipfile.ZipFile(datapath, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)"],"metadata":{"id":"tmz0ttGX7wFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os"],"metadata":{"id":"h2w-Ph9X7wCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = extract_path + '/MLQA_V1'\n","print(os.listdir(data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KAOA0rU87v__","executionInfo":{"status":"ok","timestamp":1765200713446,"user_tz":-330,"elapsed":14,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"058fa50f-237b-4ed5-b715-994d37bdb0a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['dev', 'LICENSE', 'test']\n"]}]},{"cell_type":"code","source":["data = extract_path + '/MLQA_V1/test'\n","print(os.listdir(data))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89ZiRnFC7v97","executionInfo":{"status":"ok","timestamp":1765200714681,"user_tz":-330,"elapsed":13,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"879b5dff-bc12-430b-cb8a-7e5c4cda288b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['test-context-de-question-vi.json', 'test-context-en-question-ar.json', 'test-context-ar-question-en.json', 'test-context-zh-question-hi.json', 'test-context-en-question-hi.json', 'test-context-vi-question-de.json', 'test-context-vi-question-hi.json', 'test-context-vi-question-vi.json', 'test-context-es-question-en.json', 'test-context-es-question-de.json', 'test-context-es-question-hi.json', 'test-context-hi-question-vi.json', 'test-context-vi-question-zh.json', 'test-context-de-question-hi.json', 'test-context-ar-question-es.json', 'test-context-es-question-es.json', 'test-context-hi-question-de.json', 'test-context-en-question-es.json', 'test-context-de-question-ar.json', 'test-context-es-question-ar.json', 'test-context-en-question-vi.json', 'test-context-de-question-en.json', 'test-context-ar-question-zh.json', 'test-context-ar-question-hi.json', 'test-context-es-question-zh.json', 'test-context-de-question-es.json', 'test-context-hi-question-ar.json', 'test-context-zh-question-ar.json', 'test-context-ar-question-vi.json', 'test-context-zh-question-es.json', 'test-context-en-question-zh.json', 'test-context-hi-question-en.json', 'test-context-ar-question-ar.json', 'test-context-zh-question-zh.json', 'test-context-de-question-de.json', 'test-context-en-question-en.json', 'test-context-zh-question-de.json', 'test-context-hi-question-zh.json', 'test-context-ar-question-de.json', 'test-context-zh-question-en.json', 'test-context-zh-question-vi.json', 'test-context-en-question-de.json', 'test-context-vi-question-es.json', 'test-context-vi-question-ar.json', 'test-context-es-question-vi.json', 'test-context-de-question-zh.json', 'test-context-hi-question-hi.json', 'test-context-hi-question-es.json', 'test-context-vi-question-en.json']\n"]}]},{"cell_type":"code","source":["import json"],"metadata":{"id":"H66Dpkjl7v75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Getting English Text and English Context json file\n","data = extract_path + '/MLQA_V1/test/test-context-en-question-en.json'\n","with open(data, 'r') as f:\n","    dataset = json.load(f)"],"metadata":{"id":"VPsSpDXg7v5J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset2 = json.dumps(dataset, indent=4)  # just for printing purpose dump the dataset into dataset2"],"metadata":{"id":"skoL_mn877mn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(type(dataset2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAHOBu0k77jG","executionInfo":{"status":"ok","timestamp":1765200720451,"user_tz":-330,"elapsed":6,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"1538d080-c3b1-43ae-c2a5-c5c931b74652"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'str'>\n"]}]},{"cell_type":"code","source":["print(dataset2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1W2DJepmY2_UKRQKhbFgntYNL-tcO8xAU"},"collapsed":true,"id":"1yWXrZzp77hB","executionInfo":{"status":"ok","timestamp":1765197258582,"user_tz":-330,"elapsed":94700,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"f2159f84-5ea8-448a-db33-521008972355"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["print(dataset['data'][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aZKaqBlv77eh","executionInfo":{"status":"ok","timestamp":1765200723872,"user_tz":-330,"elapsed":24,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"2f52ed39-9299-412e-f6dd-b916de4a91ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'title': 'Area 51', 'paragraphs': [{'context': 'In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"', 'qas': [{'question': 'Who analyzed the biopsies?', 'answers': [{'text': 'Rutgers University biochemists', 'answer_start': 457}], 'id': 'a4968ca8a18de16aa3859be760e43dbd3af3fce9'}, {'question': 'who represented robert frost and walter kasza in their suit?', 'answers': [{'text': 'George Washington University law professor Jonathan Turley', 'answer_start': 218}], 'id': 'f251ea56c4f1aa1df270137f7e6d89c0cc1b6ef4'}, {'question': 'What was the law suit against Groom about', 'answers': [{'text': 'the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials)', 'answer_start': 826}], 'id': '04ecd5555635bc05fd2f379d1b9027edd663cebf'}, {'question': 'what did the complainants alleged happen to them?', 'answers': [{'text': 'had sustained skin, liver, and respiratory injuries', 'answer_start': 607}], 'id': 'd066a75dbe8cd3e2b57c415a8eb54a08dc7e72a7'}]}, {'context': 'In January 2006, space historian Dwayne A. Day published an article in online aerospace magazine The Space Review titled \"Astronauts and Area 51: the Skylab Incident\". The article was based on a memo written in 1974 to CIA director William Colby by an unknown CIA official. The memo reported that astronauts on board Skylab 4 had, as part of a larger program, inadvertently photographed a location of which the memo said:', 'qas': [{'question': 'When was the aerospace magazine article published?', 'answers': [{'text': 'January 2006', 'answer_start': 3}], 'id': 'c5f545baccd8ea8adb83f75756f4832340600bd9'}]}, {'context': 'Its secretive nature and undoubted connection to classified aircraft research, together with reports of unusual phenomena, have led Area 51 to become a focus of modern UFO and other conspiracy theories. Some of the activities mentioned in such theories at Area 51 include:', 'qas': [{'question': 'What location has become an area of interest for current UFO and other conspiracy theories?', 'answers': [{'text': 'Area 51', 'answer_start': 132}], 'id': 'f9307229512ff964a59eaa408e62e05c3d10398b'}, {'question': 'What was the focus of modern UFO conspiracy?', 'answers': [{'text': 'Area 51', 'answer_start': 256}], 'id': 'fcd31d7191026816abb5b7def9c6d3fefde892bb'}]}, {'context': 'The USGS topographic map for the area only shows the long-disused Groom Mine. A civil aviation chart published by the Nevada Department of Transportation shows a large restricted area, defined as part of the Nellis restricted airspace. The National Atlas page showing federal lands in Nevada shows the area as lying within the Nellis Air Force Base. Higher resolution (and more recent) images from other satellite imagery providers (including Russian providers and the IKONOS) are commercially available. These show the runway markings, base facilities, aircraft, and vehicles.', 'qas': [{'question': 'What type of airspace is Nellis?', 'answers': [{'text': 'restricted', 'answer_start': 215}], 'id': '7390b083695d1b6d8bed9acb57a84707361c9104'}]}, {'context': 'The amount of information the United States government has been willing to provide regarding Area 51 has generally been minimal. The area surrounding the lake is permanently off-limits to both civilian and normal military air traffic. Security clearances are checked regularly; cameras and weaponry are not allowed. Even military pilots training in the NAFR risk disciplinary action if they stray into the exclusionary \"box\" surrounding Groom\\'s airspace. Surveillance is supplemented using buried motion sensors. Area 51 is a common destination for Janet, the de facto name of a small fleet of passenger aircraft operated on behalf of the United States Air Force to transport military personnel, primarily from McCarran International Airport.', 'qas': [{'question': 'What kind of action do military pilots face if they stray into prohibited areas?', 'answers': [{'text': 'disciplinary', 'answer_start': 363}], 'id': '03df1f92420416844575cfa201ae840319c40650'}]}, {'context': 'The lakebed made an ideal strip from which they could test aircraft, and the Emigrant Valley\\'s mountain ranges and the NTS perimeter, about 100 mi (160 km) north of Las Vegas, protected the test site from visitors. The CIA asked the AEC to acquire the land, designated \"Area 51\" on the map, and add it to the Nevada Test Site.Johnson named the area \"Paradise Ranch\" to encourage workers to move to a place that the CIA\\'s official history of the U-2 project would later describe as \"the new facility in the middle of nowhere\"; the name became shortened to \"the Ranch\".  On 4 May 1955, a survey team arrived at Groom Lake and laid out a 5,000-foot (1,500 m), north-south runway on the southwest corner of the lakebed and designated a site for a base support facility. \"The Ranch\", also known as Site II, initially consisted of little more than a few shelters, workshops and trailer homes in which to house its small team. In a little over three months, the base consisted of a single, paved runway, three hangars, a control tower, and rudimentary accommodations for test personnel. The base\\'s few amenities included a movie theatre and volleyball court. Additionally, there was a mess hall, several water wells, and fuel storage tanks. By July 1955, CIA, Air Force, and Lockheed personnel began arriving. The Ranch received its first U-2 delivery on 24 July 1955 from Burbank on a C-124 Globemaster II cargo plane, accompanied by Lockheed technicians on a Douglas DC-3. Regular Military Air Transport Service flights were set up between Area 51 and Lockheed\\'s Burbank, California offices. To preserve secrecy, personnel flew to Nevada on Monday mornings and returned to California on Friday evenings.', 'qas': [{'question': 'what made the testing strip for the aircraft?', 'answers': [{'text': 'The lakebed', 'answer_start': 0}], 'id': '2079cf7ce47961738e4bd0d527d0b1058210f869'}, {'question': 'who was along for the initial U-2 delivery?', 'answers': [{'text': 'Lockheed technicians', 'answer_start': 1428}], 'id': 'd5377da63e6f64dae5e269290a6334c2a912cb3f'}]}, {'context': 'The original rectangular base of 6 by 10 miles (9.7 by 16.1 km) is now part of the so-called \"Groom box\", a rectangular area measuring 23 by 25 miles (37 by 40 km), of restricted airspace. The area is connected to the internal Nevada Test Site (NTS) road network, with paved roads leading south to Mercury and west to Yucca Flat. Leading northeast from the lake, the wide and well-maintained Groom Lake Road runs through a pass in the Jumbled Hills. The road formerly led to mines in the Groom basin, but has been improved since their closure. Its winding course runs past a security checkpoint, but the restricted area around the base extends farther east. After leaving the restricted area, Groom Lake Road descends eastward to the floor of the Tikaboo Valley, passing the dirt-road entrances to several small ranches, before converging with State Route 375, the \"Extraterrestrial Highway\", south of Rachel.', 'qas': [{'question': 'What type of roads lead to the ranches?', 'answers': [{'text': 'dirt-road', 'answer_start': 775}], 'id': 'ba7865d50777f2b90ba88fcb070a672d042b6b69'}, {'question': 'Where does the Groom Lake Road head relative to the lake?', 'answers': [{'text': 'northeast', 'answer_start': 338}], 'id': 'eeb8dbd25efe5221dc6723ddee95daa07d2c8478'}]}, {'context': 'The storage, examination, and reverse engineering of crashed alien spacecraft (including material supposedly recovered at Roswell), the study of their occupants (living and dead; see grey alien), and the manufacture of aircraft based on alien technology.', 'qas': [{'question': 'What is thought to be being built in Roswell?', 'answers': [{'text': 'aircraft based on alien technology', 'answer_start': 219}], 'id': '9daf791fa16a8aa198bf467ff163b2ee3f0daf13'}]}]}\n"]}]},{"cell_type":"code","source":["print(len(dataset['data']))  # 5010 different titles (stories)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cuvB0kPl8PiC","executionInfo":{"status":"ok","timestamp":1765200725551,"user_tz":-330,"elapsed":68,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"64032130-4550-4fbe-b092-9935a106b021"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5010\n"]}]},{"cell_type":"code","source":["print(len(dataset['data'][0]['paragraphs']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5QAkpgF08Peo","executionInfo":{"status":"ok","timestamp":1765200726514,"user_tz":-330,"elapsed":10,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"68794062-2301-40c5-e1eb-aa047a8ddf6f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}]},{"cell_type":"code","source":["# For tokenization purpose we need to flatten the dataset.\n","Flat_data = {\n","    \"id\":[],\n","    \"context\":[],\n","    \"questions\":[],\n","    \"answers\":[],\n","}"],"metadata":{"id":"ZOnDMnWw-Brx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for article in dataset['data']:\n","  for paragraph in article['paragraphs']:\n","    context = paragraph['context']\n","\n","    for qa in paragraph['qas']:\n","      # Each question has context\n","      Flat_data['id'].append(qa['id'])\n","      Flat_data['context'].append(context)\n","      Flat_data['questions'].append(qa['question'])\n","      Flat_data['answers'].append(qa['answers'])"],"metadata":{"id":"fC8VjCwq8SrS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(Flat_data['questions']))\n","print(len(Flat_data['id']))\n","print(len(Flat_data['answers']))\n","print(len(Flat_data['context']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IihjGLCX8Sn7","executionInfo":{"status":"ok","timestamp":1765200734239,"user_tz":-330,"elapsed":10,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"820a401f-21e1-4585-aec4-39e09e99f72a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["11590\n","11590\n","11590\n","11590\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","from datasets import Dataset, Features, Value, Sequence"],"metadata":{"id":"29m_BKADAlIz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = 'bert-base-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["33f4fd9b534b47558b7e9a9d90eb6d06","9d2b1b6f81664186addcdef0083e7d0c","fd522a00c5194eca96841fc93ade4ccc","820c24a2565e4e05ba276bf602727885","ae72d83203b8400cb5279a4803561a24","ad83232fb8d6478bb1b5dd3ee1d673a6","9def49c537804659afcee9b594f12a2e","67ad5ef68b014259b206624e5f9d6d68","bbcf00ac21cd4d99b3b719840093c695","f42e5e505e1e4ea4be572b18efaad1b1","03b28c43ca5442c3bad90d32e1b37b74","8ac342301f5f405b972ed5d151bc8f38","7907df54d6b641ae885d38f6fb7d950a","b2c56bda18424a6db3319a6dd695a822","99d74e429fe045edad8cc1b5c804f6d3","ecda7f6a27df4c838159c52eb301d6bd","8c3228c8118c439ea9a615200daa8bb4","039f279c9cf04873a1a1be1d52ac657d","c6e8bbe4bad94489ab5004ea51bbe199","40cc997b923d4790bdf6dd2cf1fa94af","92ece4f366e1457ab43e8f7f001a077f","60d41385225a47b3b722b31bdaf4e891","f47c2300fae5493f9544c7814e08f2ca","c055ffbcbb7d4f92b416fa6e3a36b12d","e07dd857f1584c87a7ec955efffbe195","2773726b8a6446b19c09c5b50e376680","0b8209c89e7c4d04977fb650fb74a035","00d451e4468c4a2bbbd2066d0b719efb","05730c1bb5d74a8788d5618b3a497363","8aa39e39639e471aab16b91fb5062349","6248b00de0414c70b9cbef1a1fb45b10","0a7dca0eaac242b3aa27bc99aeebec56","f93497793afd4169902898a8fbd9c832","94230ce9c7414a75a46058c698595bcf","233b6c5c85ec4c40a062e282f0d2c393","ecd4d3cf138b46dd92ff786db5db129f","4f388d46d1814d9790c18636f41b6095","8a906fd8196c4788be96d5a2ee918a6b","6e109fd09ead4f279dc7a67b9ed3e925","6ada65a8b4ca46d993b80aaff3bc22cc","8ddba0eb8e5a40299d6650b54c4fc9b4","87eff7bfe7cb46e38f209ef19555dc8d","5cbefa3392f143f398706bcbd906a52d","9a056b6bb92546489a5b784ba70baea2"]},"id":"Kls49Ve3AyDW","executionInfo":{"status":"ok","timestamp":1765200747840,"user_tz":-330,"elapsed":1535,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"d21d8b37-2fc2-41fd-ab4e-e7c45df1c2c6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f4fd9b534b47558b7e9a9d90eb6d06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac342301f5f405b972ed5d151bc8f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f47c2300fae5493f9544c7814e08f2ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94230ce9c7414a75a46058c698595bcf"}},"metadata":{}}]},{"cell_type":"code","source":["def preprocess(data):\n","  tokenized = tokenizer(\n","      data['context'],\n","      data['questions'],\n","      truncation = True,\n","      max_length = 512,\n","      stride = 128,\n","      return_offsets_mapping = True,\n","      return_overflowing_tokens = True,\n","      padding = \"max_length\",\n","\n","  )\n","\n","  sample_map = tokenized.pop(\"overflow_to_sample_mapping\")\n","  offset_map = tokenized.pop(\"offset_mapping\")\n","\n","  start_pos = []\n","  end_pos = []\n","\n","  for i,offsets in enumerate(offset_map):\n","    sample_idx = sample_map[i]\n","    answer = data[\"answers\"][sample_idx][0] # Assuming there's always at least one answer and we take the first one\n","    start_idx = answer[\"answer_start\"]\n","    end_idx = start_idx + len(answer[\"text\"])\n","\n","    seq_ids = tokenized.sequence_ids(i)\n","\n","    # Find the start and end of the context (segment_id = 1)\n","    try:\n","        ctx_start = seq_ids.index(1)\n","        ctx_end = len(seq_ids) - 1 - seq_ids[::-1].index(1)\n","    except ValueError: # Handle cases where segment_id 1 (context) might not be found\n","        ctx_start = 0\n","        ctx_end = 0\n","\n","    if offsets[ctx_start][0] > start_idx or offsets[ctx_end][1] < end_idx:\n","      start_pos.append(0)\n","      end_pos.append(0)\n","      continue\n","\n","    token_start = 0\n","    token_end = 0\n","\n","    for idx in range(ctx_start, ctx_end + 1):\n","      if offsets[idx][0] <= start_idx < offsets[idx][1]:\n","        token_start = idx\n","      if offsets[idx][0] < end_idx <= offsets[idx][1]:\n","        token_end = idx\n","\n","    start_pos.append(token_start)\n","    end_pos.append(token_end)\n","  tokenized[\"start_positions\"] = start_pos\n","  tokenized[\"end_positions\"] = end_pos\n","\n","  return tokenized"],"metadata":{"id":"dSvwsGw98Sl0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the datasets and preprocessing using Dataset module\n","\n","train_dataset = Dataset.from_dict(Flat_data)\n","tokenized_train = train_dataset.map(preprocess,batched=True, remove_columns=train_dataset.column_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c740255ad56e4e18b3041c580f46e8a6","2a121671429648bca691040ec24352b2","c4db1d80c6f64acabf3751b7de83ec68","5da96aa9350b4a1f8c58fb7bda919c11","b7f88da3ce92450ba511ad42fe1041a1","b91cd6346c304dcd9edc03e7a44ad04b","5e10db69c14448a69648e3ff839f087b","7e0312b8e5b84cbebcfc2593bee0ed1c","ab09bc7b1f5341d1bc8338db11b63fc5","64188d78030542d4a71081323ad30d5c","eb7ea44631b043b2bb34951da8d85faa"]},"id":"vt-R6JOhDnKd","executionInfo":{"status":"ok","timestamp":1765201182359,"user_tz":-330,"elapsed":12738,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"b818f1ce-8c45-4d76-87fa-eb25379f338b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/11590 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c740255ad56e4e18b3041c580f46e8a6"}},"metadata":{}}]},{"cell_type":"code","source":["from torch.utils.data import DataLoader"],"metadata":{"id":"ry5w7lycJrj7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_train.set_format(\n","    type=\"torch\",\n","    columns=[\"input_ids\", \"attention_mask\",\"token_type_ids\", \"start_positions\", \"end_positions\"],\n",")\n","train_loader = DataLoader(\n","    tokenized_train,\n","    batch_size=4, # Reduced batch size to prevent OOM\n","    shuffle=True,\n",")"],"metadata":{"id":"Q49HgbzrHjGG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForQuestionAnswering\n","import torch"],"metadata":{"id":"7YzuCnb4J2r6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)\n","Model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n","Model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPdgw_y2KhZK","executionInfo":{"status":"ok","timestamp":1765201193238,"user_tz":-330,"elapsed":476,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"2ac18376-5e0d-43fd-8aaf-e4a6ad44f143"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForQuestionAnswering(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from torch.optim import AdamW"],"metadata":{"id":"y2Lfs4hxKxQB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"lPzNTKbELJ_u"}},{"cell_type":"code","source":["optimizer = AdamW(Model.parameters(), lr=3e-5)\n","\n","for epoch in range(2):\n","  Model.train()\n","\n","  for batch in train_loader:\n","    batch = {k:v.to(device) for k,v in batch.items()}\n","    outputs = Model(\n","        input_ids = batch['input_ids'],\n","        attention_mask = batch['attention_mask'],\n","        token_type_ids = batch['token_type_ids'],\n","        start_positions = batch['start_positions'].long(), # Explicitly cast to long\n","        end_positions = batch['end_positions'].long(),     # Explicitly cast to long\n","    )\n","    loss = outputs.loss\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"svh_T8TPLGwu","executionInfo":{"status":"error","timestamp":1765201471305,"user_tz":-330,"elapsed":256,"user":{"displayName":"Harshavardhan Devarakonda","userId":"01474166968225076456"}},"outputId":"38ffcab1-6f47-4bce-eddc-f87a9fc9daf0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 3092 has 14.73 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 123.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4083993741.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     outputs = Model(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1737\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1740\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    937\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 3092 has 14.73 GiB memory in use. Of the allocated memory 14.48 GiB is allocated by PyTorch, and 123.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Sk0THei9Loyq"},"execution_count":null,"outputs":[]}]}
